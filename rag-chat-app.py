import streamlit as st
import tempfile
import os
import torch
from langchain_community.document_loaders import PyPDFLoader
# from langchain_huggingface.embeddings import HuggingFaceEmbeddings
from langchain_experimental.text_splitter import SemanticChunker
from langchain_chroma import Chroma
from langchain_core.output_parsers import StrOutputParser
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
from dotenv import load_dotenv
import getpass
import time


import getpass
import os

load_dotenv()

if not os.getenv("GOOGLE_API_KEY"):
    os.environ["GOOGLE_API_KEY"] = getpass.getpass("Enter your Google API Key: ")

def initialize_session_state():
    """Initialize session state variables"""
    if 'rag_chain' not in st.session_state:
        st.session_state.rag_chain = None
    if 'models_loaded' not in st.session_state:
        st.session_state.models_loaded = False
    if 'embeddings' not in st.session_state:
        st.session_state.embeddings = None
    if 'llm' not in st.session_state:
        st.session_state.llm = None
    if 'chat_history' not in st.session_state:
        st.session_state.chat_history = []
    if 'pdf_processed' not in st.session_state:
        st.session_state.pdf_processed = False
    if 'pdf_name' not in st.session_state:
        st.session_state.pdf_name = ""

@st.cache_resource
def load_embeddings():
    embedding =  GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")
    return embedding

@st.cache_resource  
def load_llm():
    MODEL_NAME = "gemini-2.0-flash"
    return ChatGoogleGenerativeAI(model=MODEL_NAME)

def process_pdf(uploaded_file):
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
        tmp_file.write(uploaded_file.getvalue())
        tmp_file_path = tmp_file.name

    loader = PyPDFLoader(tmp_file_path)
    documents = loader.load()

    semantic_splitter = SemanticChunker(
        embeddings=st.session_state.embeddings,
        buffer_size=1,
        breakpoint_threshold_type="percentile",
        breakpoint_threshold_amount=95,
        min_chunk_size=500,
        add_start_index=True
    )

    docs = semantic_splitter.split_documents(documents)
    vector_db = Chroma.from_documents(documents=docs,
                                      embedding=st.session_state.embeddings)
    retriever = vector_db.as_retriever()

    os.unlink(tmp_file_path)
    return retriever, len(docs), docs


def get_pdf_rag_prompt():
    return ChatPromptTemplate.from_template(
        """
        System instruction: B·∫°n l√† m·ªôt tr·ª£ l√Ω AI gi√∫p tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n n·ªôi dung t√†i li·ªáu PDF.
        H√£y s·ª≠ d·ª•ng th√¥ng tin t·ª´ context sau ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi m·ªôt c√°ch ch√≠nh x√°c v√† ng·∫Øn g·ªçn.

        Context: {context}

        Question: {question}

        Answer:
        """
    )


def add_message(role, content):
    """Th√™m tin nh·∫Øn v√†o l·ªãch s·ª≠ chat"""
    st.session_state.chat_history.append({
        "role": role,
        "content": content,
        "timestamp": time.time()
    })

def clear_chat():
    """X√≥a l·ªãch s·ª≠ chat"""
    st.session_state.chat_history = []

def display_chat():
    """Hi·ªÉn th·ªã l·ªãch s·ª≠ chat"""
    if st.session_state.chat_history:
        for message in st.session_state.chat_history:
            if message["role"] == "user":
                with st.chat_message("user"):
                    st.write(message["content"])
            else:
                with st.chat_message("assistant"):
                    st.write(message["content"])
    else:
        with st.chat_message("assistant"):
            st.write("Xin ch√†o! T√¥i l√† AI assistant. H√£y upload file PDF v√† b·∫Øt ƒë·∫ßu ƒë·∫∑t c√¢u h·ªèi v·ªÅ n·ªôi dung t√†i li·ªáu nh√©! üòä")

# UI
def main():
    initialize_session_state()

    st.set_page_config(
        page_title="PDF RAG Chatbot", 
        layout="wide",
        initial_sidebar_state="expanded"
    )
    st.title("PDF RAG Assistant")
    st.logo("./aibybit_logo.png", size="large")
    
    # Sidebar
    with st.sidebar:
        st.title("‚öôÔ∏è C√†i ƒë·∫∑t")
        
        # Load models
        if not st.session_state.models_loaded:
            st.warning("‚è≥ ƒêang t·∫£i models...")
            with st.spinner("ƒêang t·∫£i AI models..."):
                st.session_state.embeddings = load_embeddings()
                st.session_state.llm = load_llm()
                st.session_state.models_loaded = True
            st.success("‚úÖ Models ƒë√£ s·∫µn s√†ng!")
            st.rerun()
        else:
            st.success("‚úÖ Models ƒë√£ s·∫µn s√†ng!")

        st.markdown("---")
        
        # Upload PDF
        st.subheader("üìÑ Upload t√†i li·ªáu")
        uploaded_files = st.file_uploader("Upload file PDF", type="pdf", accept_multiple_files=True)
        
        if uploaded_files and st.button("X·ª≠ l√Ω PDF"):
            try:
                with st.spinner("ƒêang x·ª≠ l√Ω PDF..."):
                    # X·ª≠ l√Ω nhi·ªÅu PDF
                    all_chunks = []
                    for uploaded_file in uploaded_files:
                        retriever, num_chunks, chunks = process_pdf(uploaded_file)
                        all_chunks.extend(chunks)

                    # T·∫°o vector database t·ª´ t·∫•t c·∫£ chunks
                    vector_db = Chroma.from_documents(
                        documents=all_chunks,
                        embedding=st.session_state.embeddings
                    )
                    retriever = vector_db.as_retriever()

                    st.session_state.rag_chain = retriever
                    st.session_state.pdf_processed = True
                    st.session_state.pdf_name = ", ".join([f.name for f in uploaded_files])
                    st.session_state.chunks = all_chunks

                    # Hi·ªÉn th·ªã th√¥ng b√°o
                    message = f"""
                    ‚úÖ ƒê√£ x·ª≠ l√Ω th√†nh c√¥ng file **{st.session_state.pdf_name}**!
                    T√†i li·ªáu ƒë∆∞·ª£c chia th√†nh {len(st.session_state.chunks)} ph·∫ßn.
                    B·∫°n c√≥ th·ªÉ b·∫Øt ƒë·∫ßu ƒë·∫∑t c√¢u h·ªèi v·ªÅ n·ªôi dung t√†i li·ªáu.
                    """

                    clear_chat()
                    add_message("assistant", message)
                    st.rerun()
            except Exception as e:
                st.error(f"L·ªói khi x·ª≠ l√Ω PDF: {str(e)}")
        
        # PDF status
        if st.session_state.pdf_processed:
            st.success(f"üìÑ ƒê√£ t·∫£i: {st.session_state.pdf_name}")
        else:
            st.info("üìÑ Ch∆∞a c√≥ t√†i li·ªáu")
            
        st.markdown("---")
        
        # Chat controls
        st.subheader("üí¨ ƒêi·ªÅu khi·ªÉn Chat")
        if st.button("üóëÔ∏è X√≥a l·ªãch s·ª≠ chat", use_container_width=True):
            clear_chat()
            st.rerun()
            
        st.markdown("---")
        
        # Instructions
        st.subheader("üìã H∆∞·ªõng d·∫´n")
        st.markdown("""
        **C√°ch s·ª≠ d·ª•ng:**
        1. **Upload PDF** - Ch·ªçn file v√† nh·∫•n "X·ª≠ l√Ω PDF"
        2. **ƒê·∫∑t c√¢u h·ªèi** - Nh·∫≠p c√¢u h·ªèi trong √¥ chat
        3. **Nh·∫≠n tr·∫£ l·ªùi** - AI s·∫Ω tr·∫£ l·ªùi d·ª±a tr√™n n·ªôi dung PDF
        """)

    # Main content
    st.markdown("*Tr√≤ chuy·ªán v·ªõi Chatbot ƒë·ªÉ trao ƒë·ªïi v·ªÅ n·ªôi dung t√†i li·ªáu PDF c·ªßa b·∫°n*")
    
    # Chat container
    chat_container = st.container()
    
    with chat_container:
        # Display chat history
        display_chat()
    
    # Chat input
    if st.session_state.models_loaded and st.session_state.rag_chain:
        if st.session_state.pdf_processed:
            # User input
            user_input = st.chat_input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n...")
            
            if user_input:
                # Add user message
                add_message("user", user_input)
                
                # Display user message immediately
                with st.chat_message("user"):
                    st.write(user_input)
                
                # Generate response
                with st.chat_message("assistant"):
                    with st.spinner("ƒêang suy nghƒ©..."):
                        try:
                            relevant_docs = st.session_state.rag_chain.invoke(user_input)
                            context = "\n\n".join([doc.page_content for doc in relevant_docs])

                            prompt = get_pdf_rag_prompt()
                            output_parser = StrOutputParser()
                            chain = prompt | st.session_state.llm | output_parser

                            answer = chain.invoke({"context": context, "question": user_input})

                            st.markdown(f"**Answer**: {answer}")
                            # Clean up the response
                            # if 'Answer:' in answer:
                            #     answer = answer.split('Answer:')[0].strip()
                            
                            # Add assistant message to history
                            add_message("assistant", answer)
                            
                        except Exception as e:
                            error_msg = f"Xin l·ªói, ƒë√£ c√≥ l·ªói x·∫£y ra: {str(e)}"
                            st.error(error_msg)
                            add_message("assistant", error_msg)
        else:
            st.info("üîÑ Vui l√≤ng upload v√† x·ª≠ l√Ω file PDF tr∆∞·ªõc khi b·∫Øt ƒë·∫ßu chat!")
            st.chat_input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n...", disabled=True)
    else:
        st.info("‚è≥ ƒêang t·∫£i AI models, vui l√≤ng ƒë·ª£i...")
        st.chat_input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n...", disabled=True)

if __name__ == "__main__":
    main()