# Retail Forecasting ‚Äì Module 4

## M√¥ t·∫£

D·ª± √°n n√†y th·ª±c hi·ªán ph√¢n t√≠ch to√†n di·ªán d·ªØ li·ªáu b√°n l·∫ª tr·ª±c tuy·∫øn t·ª´ b·ªô d·ªØ li·ªáu "Online Retail II" c·ªßa UCI Machine Learning Repository. B·ªô d·ªØ li·ªáu ch·ª©a th√¥ng tin giao d·ªãch t·ª´ m·ªôt c√¥ng ty b√°n l·∫ª qu√† t·∫∑ng tr·ª±c tuy·∫øn t·∫°i UK v·ªõi h∆°n 1 tri·ªáu b·∫£n ghi t·ª´ th√°ng 12/2009 ƒë·∫øn 12/2011.

### C√°c b∆∞·ªõc ƒë√£ ho√†n th√†nh ‚úÖ

#### üîÑ **B∆∞·ªõc 1: Data Cleaning & Preprocessing**
- ƒê·ªçc v√† x·ª≠ l√Ω d·ªØ li·ªáu th√¥ t·ª´ file `data_online_retail_II.csv`
- X·ª≠ l√Ω missing values (Description, Customer ID)
- EDA c∆° b·∫£n: th·ªëng k√™ m√¥ t·∫£, ph√¢n ph·ªëi, missing values
- T·∫°o d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω: `processed_data_gross.csv`

#### üìä **B∆∞·ªõc 2: Exploratory Data Analysis (EDA) Chi Ti·∫øt**
- Ph√¢n t√≠ch to√†n di·ªán v·ªõi 15 b∆∞·ªõc chi ti·∫øt tr√™n d·ªØ li·ªáu gross
- Schema analysis v√† data quality assessment
- Business cleaning v√† time series chu·∫©n h√≥a
- Ph√¢n t√≠ch m·ª•c ti√™u Quantity v√† ph√¢n l·ªõp nhu c·∫ßu theo SKU (ADI & CV¬≤)
- M√πa v·ª•, gi√° c·∫£, kh√°ch h√†ng, outliers analysis
- Time-based splits v√† leakage prevention
- Essential charts v√† feature suggestions
- Quality control dashboards

#### üèóÔ∏è **B∆∞·ªõc 3: Feature Engineering (leakage-safe, weekly W-MON)**
- Resample weekly theo `W-MON`, reindex li√™n t·ª•c t·ª´ng SKU, fill-0 tu·∫ßn kh√¥ng b√°n
- Snapshot TRAIN-only: `median_price_sku`, `p99_qty_sku`
- ƒê·∫∑c tr∆∞ng th·ªùi gian d√πng d·ªØ li·ªáu t‚àí1 (tr√°nh leakage):
  - Lags: `qty_lag_1..5,8,12`
  - Rolling/EMA: t√≠nh t·ª´ `qty_prev = Quantity.shift(1)`
  - Intermittency: `weeks_since_last_sale` t·ª´ `Quantity.shift(1)`, `zero_rate_*w` tr√™n `qty_prev`
  - Gi√°/khuy·∫øn m√£i: `price_prev = Price.shift(1)`, `price_index = price_prev/median_price_sku`,
    `d_price_pct_1w = price_prev vs price_prev_prev`, rolling/z-score tr√™n `price_prev`
  - C·ªù ch·∫•t l∆∞·ª£ng: `extreme_qty_flag` d√πng `qty_lag_1 > p99_qty_sku`, `price_jump_flag` d·ª±a tr√™n `price_prev`
- T·∫°o file: `fe_weekly_gross.csv`

#### ü§ñ **B∆∞·ªõc 4: Modeling (Two-stage v·ªõi LightGBM & XGBoost)**
- Two-stage approach: Classification (has_sale) + Regression (quantity khi c√≥ sale)
- So s√°nh LightGBM vs XGBoost (baseline vs regularized)
- Split: train ‚â§ 2011-06-01, val 2011-06‚Üí2011-09, test 2011-09‚Üí2011-12
- Metrics:
  - Classification: Accuracy, Precision, Recall, F1 (VAL/TEST)
  - Regression: MAE, RMSE, WAPE, SMAPE (TEST)
- Visualizations:
  - Time series overlay (True vs d·ª± b√°o) theo SKU tr√™n VAL/TEST
  - Barplot t·ªïng h·ª£p metrics v√† ph√¢n ph·ªëi residual tr√™n TEST

#### üî¨ **B∆∞·ªõc 5: XAI - Explainable AI v·ªõi SHAP**
- SHAP analysis cho c·∫£ Classification v√† Regression models
- Global feature importance v√† feature impact distributions
- Dependency plots ƒë·ªÉ hi·ªÉu m·ªëi quan h·ªá gi·ªØa features
- Local explanations cho c√°c samples c·ª• th·ªÉ
- Business insights t·ª´ feature importance analysis

## C√†i ƒë·∫∑t

```bash
# C√†i ƒë·∫∑t dependencies
pip install -r requirements.txt

# Ho·∫∑c s·ª≠ d·ª•ng uv (n·∫øu c√≥)
uv sync

# Ki·ªÉm tra setup v√† d·ªØ li·ªáu
python test_notebook_setup.py

# K√≠ch ho·∫°t kernel venv (ƒë√£ ƒëƒÉng k√Ω)
jupyter kernelspec list
# Ch·ªçn kernel: Python (.venv Retail Forecasting)

# Ch·∫°y pipeline
jupyter notebook notebooks/03-Feature-Engineering.ipynb
jupyter notebook notebooks/04-Modeling.ipynb
```

## C·∫•u tr√∫c d·ª± √°n

```
project-4.2-Retail-Forecasting/
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îú‚îÄ‚îÄ 01-Data-Cleaning-and-EDA.ipynb        # Data cleaning & EDA c∆° b·∫£n
‚îÇ   ‚îú‚îÄ‚îÄ 02-EDA-Processed-Data-Gross.ipynb     # EDA chi ti·∫øt (15 b∆∞·ªõc)
‚îÇ   ‚îú‚îÄ‚îÄ 03-Feature-Engineering.ipynb          # FE leakage-safe (weekly)
‚îÇ   ‚îú‚îÄ‚îÄ 04-Modeling.ipynb                     # Two-stage LGBM/XGB + so s√°nh
‚îÇ   ‚îú‚îÄ‚îÄ 05-XAI-Explainable-AI.ipynb          # SHAP analysis & business insights
‚îÇ   ‚îî‚îÄ‚îÄ reports/                              # B√°o c√°o v√† h√¨nh ·∫£nh
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ data_online_retail_II.csv         # D·ªØ li·ªáu g·ªëc (1M+ records)
‚îÇ   ‚îî‚îÄ‚îÄ processed/
‚îÇ       ‚îú‚îÄ‚îÄ processed_data_gross.csv          # D·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω (1M+ records)
‚îÇ       ‚îú‚îÄ‚îÄ fe_weekly_gross.csv               # Weekly FE (4863 SKUs, 106 weeks)
‚îÇ       ‚îú‚îÄ‚îÄ snapshots/
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ snap_gross_train_stats.csv    # median_price & p99 by SKU (TRAIN-only)
‚îÇ       ‚îî‚îÄ‚îÄ predictions/
‚îÇ           ‚îú‚îÄ‚îÄ test_predictions_lgb_xgb.csv  # D·ª± b√°o test (LGBM/XGB)
‚îÇ           ‚îú‚îÄ‚îÄ val_predictions_lgb_xgb.csv   # D·ª± b√°o validation (LGBM/XGB)
‚îÇ           ‚îú‚îÄ‚îÄ metrics_summary.csv
‚îÇ           ‚îú‚îÄ‚îÄ classification_metrics_summary.csv
‚îÇ           ‚îú‚îÄ‚îÄ classification_metrics_comparison_regularized.csv
‚îÇ           ‚îî‚îÄ‚îÄ regression_metrics_comparison_regularized.csv
‚îú‚îÄ‚îÄ requirements.txt                          # Dependencies
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ       ‚îî‚îÄ‚îÄ plots.py                          # H√†m v·∫Ω: SKU overlay & grid
‚îî‚îÄ‚îÄ README.md
```

## Ki·ªÉm tra v√† ch·∫°y

### üîç **Ki·ªÉm tra nhanh d·ªØ li·ªáu**
```bash
# Ch·∫°y ki·ªÉm tra to√†n di·ªán d·ªØ li·ªáu
python quick_checks.py

# Ki·ªÉm tra setup notebook
python test_notebook_setup.py
```

### üìä **Ch·∫°y pipeline d·ª± b√°o**

```bash
# 1) Data Cleaning & EDA c∆° b·∫£n (t·∫°o processed_data_gross.csv)
jupyter notebook notebooks/01-Data-Cleaning-and-EDA.ipynb

# 2) EDA chi ti·∫øt (15 b∆∞·ªõc ph√¢n t√≠ch to√†n di·ªán)
jupyter notebook notebooks/02-EDA-Processed-Data-Gross.ipynb

# 3) Feature Engineering (t·∫°o fe_weekly_gross.csv)
jupyter notebook notebooks/03-Feature-Engineering.ipynb

# 4) Modeling hai b∆∞·ªõc (LGBM & XGB)
jupyter notebook notebooks/04-Modeling.ipynb

# 5) XAI - Explainable AI v·ªõi SHAP
jupyter notebook notebooks/05-XAI-Explainable-AI.ipynb
```

## üìä **Script EDA Chi Ti·∫øt (eda_detailed_explanations.py)**

Script n√†y cung c·∫•p b√°o c√°o EDA ho√†n ch·ªânh v·ªõi 15 b∆∞·ªõc ph√¢n t√≠ch chi ti·∫øt:

### üéØ **15 B∆∞·ªõc EDA v·ªõi ch√∫ th√≠ch ƒë·∫ßy ƒë·ªß:**

1. **Ch·ªçn ph·∫°m vi & h·∫°t d·ªØ li·ªáu**
   - Cutoff date, forecast horizon, dominant country detection
   - Gi·∫£i th√≠ch t·∫°i sao ch·ªçn weekly granularity

2. **Ki·ªÉm k√™ schema & ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu**
   - Schema table v·ªõi %missing
   - Duplicates analysis v·ªõi gi·∫£i th√≠ch business meaning
   - Anomalies detection v·ªõi recommendations

3. **L√†m s·∫°ch nghi·ªáp v·ª• (pre-clean)**
   - Technical codes, returns, bad prices
   - Business rules cho cleaning

4. **Chu·∫©n h√≥a chu·ªói th·ªùi gian**
   - Weekly aggregation, zero-filling
   - Attributes calculation v·ªõi √Ω nghƒ©a

5. **Kh√°m ph√° m·ª•c ti√™u (Quantity)**
   - Distribution analysis, quantiles
   - Rolling statistics v·ªõi insights

6. **Ph√¢n l·ªõp nhu c·∫ßu theo SKU**
   - ADI & CV¬≤ classification
   - Smooth/Intermittent/Erratic/Lumpy v·ªõi recommendations

7. **M√πa v·ª• & chu k·ª≥**
   - Week-of-year heatmap analysis
   - Seasonal patterns insights

8. **Gi√° & khuy·∫øn m√£i**
   - Price elasticity analysis
   - Promotion detection v·ªõi business insights

9. **Kh√°ch h√†ng & gi·ªè h√†ng**
   - Customer loyalty analysis
   - Market basket patterns

10. **Outliers & anomalies**
    - Statistical outlier detection
    - Business rules cho outlier handling

11. **Ph√¢n kh√∫c theo Country**
    - Geographic comparison
    - Bias detection v√† recommendations

12. **Split theo th·ªùi gian & ch·ªëng r√≤ r·ªâ**
    - Time-based splits
    - Leakage prevention checklist

13. **Essential Summary Charts**
    - Calendar heatmap Qty theo ng√†y/tu·∫ßn (to√†n b·ªô)
    - Top-50 SKU √ó tu·∫ßn heatmap (chu·∫©n h√≥a theo SKU)
    - Scatter log Qty vs log Price (m·∫´u ng·∫´u nhi√™n c√≥ tr·ªçng s·ªë)
    - Bar zero-rate, returns-rate, promo-rate theo SKU (top-30)

14. **Feature Suggestions**
    - Feature importance preview chart (n·∫øu ch·∫°y nhanh m√¥ h√¨nh baseline) ho·∫∑c correlogram gi·ªØa candidate features (lag, rolling, promo, price change‚Ä¶)
    - Lag profile plot: t∆∞∆°ng quan Qty_t v·ªõi Qty_{t‚àík} (k=1..12) theo nh√≥m SKU

15. **Deliverables & Quality Control**
    - Data quality dashboard: tiles hi·ªÉn th·ªã %NaN, %duplicates, #SKU, #weeks covered‚Ä¶
    - Anomaly audit table (SKU, tu·∫ßn, lo·∫°i flag, quy·∫øt ƒë·ªãnh x·ª≠ l√Ω)
    - SKU summary table: l·ªãch s·ª≠ (tu·∫ßn), ADI, CV¬≤, zero-rate, p95/p99 Qty, median price, promo-rate, returns-rate
    - Action tracker table: m·ªói quy·∫øt ƒë·ªãnh (keep/flag/winsorize/drop), l√Ω do, ·∫£nh h∆∞·ªüng %Qty/%revenue

### üéâ **K·∫øt qu·∫£ th·ª±c t·∫ø t·ª´ script:**

- **Schema analysis** v·ªõi missing values v√† data types
- **Duplicates detection** (1.035% logic duplicates)
- **Anomalies** (1.72% returns rate)
- **SKU classification** (61.5% lumpy, 37.9% erratic, 0.6% smooth)
- **Seasonal patterns** t·ª´ heatmap analysis
- **Price elasticity** v√† promotion effects
- **Geographic differences** gi·ªØa countries
- **Time splits** v·ªõi leakage check
- **Essential summary charts** v·ªõi heatmap v√† scatter plots
- **Feature suggestions** cho modeling pipeline
- **Quality control dashboards** v√† audit tables

## S·ª≠ d·ª•ng d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω

### 1. **D·ªØ li·ªáu ch√≠nh ƒë√£ x·ª≠ l√Ω**
File: `data/processed/processed_data_gross.csv`
- Ch·ª©a 1,002,894 b·∫£n ghi ƒë√£ ƒë∆∞·ª£c l√†m s·∫°ch (sales only, gross data)
- Bao g·ªìm c√°c c·ªôt: Invoice, StockCode, Description, Quantity, InvoiceDate, Price, Customer ID, Country, InvoiceMonth, InvoiceQuarter, InvoiceYear, IsExistID, Country_encoded, Revenue

### 2. **D·ªØ li·ªáu Feature Engineered (Weekly)**
File: `data/processed/fe_weekly_gross.csv`
- D·ªØ li·ªáu weekly theo SKU (4,863 SKUs, 106 weeks)
- Bao g·ªìm features leakage-safe: lags, rolling, EMA, price features, intermittency
- Target: `y` (quantity), `label_has_sale` (binary classification)

### 3. **Snapshot th·ªëng k√™ (TRAIN-only)**
File: `data/processed/snapshots/snap_gross_train_stats.csv`
- median_price_sku v√† p99_qty_sku theo t·ª´ng SKU (t√≠nh t·ª´ TRAIN data only)
- D√πng ƒë·ªÉ tr√°nh leakage trong feature engineering

### 4. **K·∫øt qu·∫£ d·ª± b√°o**
Files trong `data/processed/predictions/`:
- `test_predictions_lgb_xgb.csv`: D·ª± b√°o tr√™n test set
- `val_predictions_lgb_xgb.csv`: D·ª± b√°o tr√™n validation set
- C√°c file metrics summary v√† comparison

### 5. **H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng cho modeling**

```python
import pandas as pd
import numpy as np

# ƒê·ªçc d·ªØ li·ªáu FE
fe = pd.read_csv('data/processed/fe_weekly_gross.csv', parse_dates=['InvoiceDate'])
fe['label_has_sale'] = (fe['y'] > 0).astype(int)

# No-leakage features (lo·∫°i b·ªè c√°c c·ªôt c√≥ th·ªÉ g√¢y leakage)
drop_cols = {'InvoiceDate','StockCode','Country','y','label_has_sale','Quantity','Price'}
feature_cols = [c for c in fe.columns if c not in drop_cols]

# Time-based splits (leakage-safe)
TRAIN_END = pd.Timestamp('2011-06-01')
VAL_END = pd.Timestamp('2011-09-01')

train_idx = fe['InvoiceDate'] < TRAIN_END
val_idx = (fe['InvoiceDate'] >= TRAIN_END) & (fe['InvoiceDate'] < VAL_END)
test_idx = fe['InvoiceDate'] >= VAL_END

# Training data
X_train = fe.loc[train_idx, feature_cols]
y_train_cls = fe.loc[train_idx, 'label_has_sale']
y_train_reg = fe.loc[train_idx, 'y']

# Validation/Test data
X_val = fe.loc[val_idx, feature_cols]
y_val_cls = fe.loc[val_idx, 'label_has_sale']
X_test = fe.loc[test_idx, feature_cols]
y_test_cls = fe.loc[test_idx, 'label_has_sale']
```

## Y√™u c·∫ßu h·ªá th·ªëng

- Python >= 3.8
- RAM: T·ªëi thi·ªÉu 8GB (khuy·∫øn ngh·ªã 16GB cho d·ªØ li·ªáu l·ªõn)
- Storage: T·ªëi thi·ªÉu 5GB ƒë·ªÉ l∆∞u tr·ªØ d·ªØ li·ªáu v√† k·∫øt qu·∫£

## C√°c th∆∞ vi·ªán ch√≠nh

- **pandas**: X·ª≠ l√Ω v√† ph√¢n t√≠ch d·ªØ li·ªáu
- **numpy**: T√≠nh to√°n s·ªë h·ªçc
- **matplotlib/seaborn/plotly**: Tr·ª±c quan h√≥a
- **scikit-learn**: Machine learning v√† evaluation metrics
- **lightgbm**: Gradient boosting framework
- **xgboost**: Gradient boosting framework (native DMatrix API)
- **jupyter**: M√¥i tr∆∞·ªùng ph√¢n t√≠ch t∆∞∆°ng t√°c
